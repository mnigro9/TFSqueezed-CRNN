{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqcwOjv63tlT",
        "colab_type": "code",
        "outputId": "548f28f2-3aa9-412d-ba07-75508f2f0ac8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AuOBmrQDg6FW",
        "colab_type": "text"
      },
      "source": [
        "# Processing new data into the training and testing sets. Do once, then save splits in .npz files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPkxE92DdzF3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# only need if error occurs for pickled file\n",
        "!pip install numpy==1.16.2  #need for loading pickled file\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vW9hfXYB0VpS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "datapath = '/content/drive/My Drive/DCASE19 ASC/LogTFSqueezedDoubleEmp/'  #set path to files\n",
        "\n",
        "# using the DCASE 2019 train and evaluate data splits\n",
        "trainpath = '/content/drive/My Drive/DCASE19 ASC/TAU-urban-acoustic-scenes-2019-development/evaluation_setup/fold1_train.csv'\n",
        "testpath = '/content/drive/My Drive/DCASE19 ASC/TAU-urban-acoustic-scenes-2019-development/evaluation_setup/fold1_evaluate.csv'\n",
        "\n",
        "import csv\n",
        "import os\n",
        "import numpy as np\n",
        "training_set=[]\n",
        "training_labels=[]\n",
        "i=0\n",
        "with open(trainpath,'r') as t:\n",
        "  reader = csv.reader(t,delimiter='\\t')\n",
        "  next(reader)\n",
        "  for row in reader:\n",
        "    #filename=row[0]\n",
        "    filename, _ = os.path.splitext(os.path.basename(row[0]))\n",
        "    print(i,filename)\n",
        "    temp = np.load(datapath+filename+'.npy') #loads file\n",
        "    training_set.append(temp)\n",
        "    training_labels.append(row[1])\n",
        "    i=i+1\n",
        "\n",
        "#if program freezes on the first file loading, restart runtime"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pH_pKCB3c7h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "testing_set=[]\n",
        "testing_labels=[]\n",
        "i=0\n",
        "with open(testpath,'r') as t:\n",
        "  reader = csv.reader(t,delimiter='\\t')\n",
        "  next(reader)\n",
        "  for row in reader:\n",
        "    #filename=row[0]\n",
        "    filename, _ = os.path.splitext(os.path.basename(row[0]))\n",
        "    print(i,filename)\n",
        "    temp = np.load(datapath+filename+'.npy') #loads file\n",
        "    testing_set.append(temp)\n",
        "    testing_labels.append(row[1])\n",
        "    i=i+1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8cEc20-fdE1",
        "colab_type": "code",
        "outputId": "7238af2d-7bac-434a-b780-f93d4cf534b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "testing_set[7].shape  # 1.(128,501), "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(128, 501)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTtMlRDilTRN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###### used only for mel data as some are of different dimensions (128,501) vs (128,500)\n",
        "# process the data into array format. reshape to fit NN input shape required\n",
        "\n",
        "def even_out_list2array(training_set):   #use for melspectrogram data. some are uneven size (128,501) vs (128,500), remove 1 sample from extras\n",
        "  #lst_=[]\n",
        "  for i in range(len(training_set)):\n",
        "    training_set[i] = training_set[i][0:128,0:500]\n",
        "    #lst_.append(training_set)\n",
        "  #X = np.asarray(lst_)\n",
        "  #del lst_\n",
        "  X = np.asarray(training_set)\n",
        "  print(type(X))\n",
        "  X = np.reshape(X, X.shape + (1,))\n",
        "  print(X.shape)\n",
        "  return X\n",
        "\n",
        "X = even_out_list2array(training_set)  \n",
        "X_test = even_out_list2array(testing_set)  \n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIHiYkE2uDwD",
        "colab_type": "code",
        "outputId": "39ca8122-8877-48b2-dba7-dd30f7174c1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# fix input representations as proper datatype and shape\n",
        "X=np.asarray(training_set)\n",
        "#print(type(X))\n",
        "X_test = np.array(testing_set)\n",
        "#print(X[0].shape)\n",
        "X = np.reshape(X, X.shape + (1,))  #reshape to (freq, time, 1) for Conv2D input\n",
        "X_test = np.reshape(X_test,X_test.shape + (1,))\n",
        "#print(X[0].shape)\n",
        "print(X_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4185, 117, 234, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLX7PZGugwgP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save the train and test data splits to drive file \n",
        "savepath = '/content/drive/My Drive/ICASSP_proj_data/'\n",
        "np.savez(savepath+'training_set_LogTFSqueezedDoubleEmp', X=X, training_labels=training_labels)\n",
        "np.savez(savepath+'testing_set_LogTFSqueezedDoubleEmp', X_test=X_test, testing_labels=testing_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLvkrcpaSK5m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# check files loaded properly by plot\n",
        "tmppsd = []\n",
        "sampleaddrs=[] \n",
        "for i in range(0,10):\n",
        "  tmppsd.append(X[np.argmax(Y==i)])  #find first instance where argument is true, return index\n",
        "  sampleaddrs.append(training_labels[np.argmax(Y==i)])\n",
        "  print(training_labels[np.argmax(Y==i)])\n",
        "\n",
        "f = plt.figure(figsize=(50,10))\n",
        "for i in range(0,10):\n",
        "  psd = np.reshape(tmppsd[i], (128,500)) # (117,234))\n",
        "  ax = f.add_subplot(5,2, i+1)\n",
        "  ax.imshow(psd[0:78],cmap='gray_r', vmin=0, vmax=255, origin='lower')\n",
        "  plt.title(sampleaddrs[i])\n",
        "  #plt.subplot(5,2, i+1)\n",
        "  #plt.imshow(psd[0:100],cmap='gray_r', vmin=0, vmax=255, origin='lower')\n",
        "  #plt.title(sampleaddrs[i])\n",
        "  \n",
        "\n",
        "#f.subplots_adjust(hspace=2, wspace=0.3)\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95EpBMJAgxf-",
        "colab_type": "text"
      },
      "source": [
        "# Begin"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuMKF8yncCGJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWu07rVZiaiA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt \n",
        "def confmat(cm, classes, normalize=False, title=None, cmap=plt.cm.Blues):  #use for nice plot of confusion matrix\n",
        "  if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "  else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "        \n",
        "        \n",
        "  fig, ax = plt.subplots()\n",
        "  im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "  ax.figure.colorbar(im, ax=ax)\n",
        "  # We want to show all ticks...\n",
        "  ax.set(xticks=np.arange(cm.shape[1]),\n",
        "         yticks=np.arange(cm.shape[0]),\n",
        "         # ... and label them with the respective list entries\n",
        "         xticklabels=classes, yticklabels=classes,\n",
        "         title=title,\n",
        "         ylabel='True label',\n",
        "         xlabel='Predicted label')\n",
        "\n",
        "  # Rotate the tick labels and set their alignment.\n",
        "  plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
        "           rotation_mode=\"anchor\")\n",
        "\n",
        "  # Loop over data dimensions and create text annotations.\n",
        "  fmt = '.2f' if normalize else 'd'\n",
        "  thresh = cm.max() / 2.\n",
        "  for i in range(cm.shape[0]):\n",
        "      for j in range(cm.shape[1]):\n",
        "          ax.text(j, i, format(cm[i, j], fmt),\n",
        "                  ha=\"center\", va=\"center\",\n",
        "                  color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "  fig.tight_layout()\n",
        "  return ax\n",
        "\n",
        "def plot_loss(history):\n",
        "  plt.subplot(1,2,1)\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title('model loss')\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train','validation'], loc='upper right')\n",
        "  plt.subplot(1,2,2)\n",
        "  plt.plot(history.history['acc'])\n",
        "  plt.plot(history.history['val_acc'])\n",
        "  plt.title('model acc')\n",
        "  plt.ylabel('acc')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train','validation'], loc='lower right')\n",
        "  plt.tight_layout()\n",
        "  plt.show()  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoU5RXvVT0wa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def stats(mdl,mdlname,X_test):\n",
        "  if mdlname:\n",
        "    mdl.load_weights('/content/drive/My Drive/DCASE19 ASC/checkpoints/'+str(mdlname)+'.hdf5')\n",
        "  from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
        "\n",
        "  yhat_d = mdl.predict(X_test)\n",
        "  pred_d = yhat_d.argmax(axis=1)\n",
        "  acc = accuracy_score(Y_test,pred_d)*100\n",
        "  rec = recall_score(Y_test,pred_d,average='weighted')\n",
        "  pre = precision_score(Y_test,pred_d,average='weighted')\n",
        "  f = f1_score(Y_test,pred_d,average='weighted')\n",
        "\n",
        "  return acc, rec, pre, f\n",
        "  '''\n",
        "  print(accuracy_score(Y_test,pred_d)*100)\n",
        "  print(classification_report(Y_test,pred_d,target_names=labels))\n",
        "  cm_d = confusion_matrix(Y_test,pred_d)\n",
        "  confmat(cm_d, labels)\n",
        "  '''\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWT2abs2vz6-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ci(data):  #determine 95% confidence interval \n",
        "  m = np.mean(data)\n",
        "  s = np.std(data)\n",
        "  from math import sqrt\n",
        "  c = 1.96 *(s/sqrt(len(data)))  #95% confidence interval\n",
        "  print('average',m,'with confidence interval',c)\n",
        "  return c "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbaLNbPbUd6a",
        "colab_type": "code",
        "outputId": "36b16a7e-0483-422c-c3b1-ea4ba6936b4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# load variable files of train and test set\n",
        "\n",
        "import numpy as np\n",
        "emp =3\n",
        "if emp==1:\n",
        "  tr = '/content/drive/My Drive/ICASSP_proj_data/training_set_TFSqueezedSingleEmp.npz'\n",
        "  te = '/content/drive/My Drive/ICASSP_proj_data/testing_set_TFSqueezedSingleEmp.npz'\n",
        "elif emp==2:\n",
        "  tr = '/content/drive/My Drive/ICASSP_proj_data/training_set_TFSqueezedDoubleEmp.npz'\n",
        "  te = '/content/drive/My Drive/ICASSP_proj_data/testing_set_TFSqueezedDoubleEmp.npz'\n",
        "elif emp==3:\n",
        "  tr = '/content/drive/My Drive/ICASSP_proj_data/training_set_LogTFSqueezedDoubleEmp.npz'\n",
        "  te = '/content/drive/My Drive/ICASSP_proj_data/testing_set_LogTFSqueezedDoubleEmp.npz'\n",
        "else:\n",
        "  tr = '/content/drive/My Drive/ICASSP_proj_data/training_set_NormalPSD.npz'#PSDdB.npz'  #NormalPSD.npz'\n",
        "  te = '/content/drive/My Drive/ICASSP_proj_data/testing_set_NormalPSD.npz'#PSDdB.npz'  #NormalPSD.npz'\n",
        "\n",
        "\n",
        "npzfile1 = np.load(tr)\n",
        "X = npzfile1['X']  #training features\n",
        "training_labels=npzfile1['training_labels']\n",
        "\n",
        "npzfile2 = np.load(te)\n",
        "X_test = npzfile2['X_test']\n",
        "testing_labels = npzfile2['testing_labels']\n",
        "print(X.shape)\n",
        "print(tr)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(9185, 117, 234, 1)\n",
            "/content/drive/My Drive/ICASSP_proj_data/training_set_LogTFSqueezedDoubleEmp.npz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAgyturg_XZL",
        "colab_type": "code",
        "outputId": "37f58ab3-830b-415c-9a7f-f34b1f3758f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# checking data formatting \n",
        "print(type(X))\n",
        "X = np.asarray(X)\n",
        "X_test = np.asarray(X_test)\n",
        "print(type(X))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "et95zdaX2ZRb",
        "colab_type": "code",
        "outputId": "2bc6dcf8-a3cb-4063-c273-f5820858ef54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# process the data labels. one hot encoding\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "Y=np.array(training_labels)\n",
        "label_encoder = LabelEncoder()  #text to numeric label. alphabetical order\n",
        "Y = label_encoder.fit_transform(Y)\n",
        "\n",
        "Y_test=np.array(testing_labels)\n",
        "Y_test = label_encoder.fit_transform(Y_test)\n",
        "\n",
        "labels=np.unique(training_labels)\n",
        "\n",
        "# checking the text label to integer to categorical worked\n",
        "print(labels)\n",
        "from keras.utils import to_categorical\n",
        "Y_cat = to_categorical(Y)\n",
        "Y_test_cat = to_categorical(Y_test)\n",
        "print(Y_cat[3])\n",
        "print(Y[3])\n",
        "print(training_labels[3])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['airport' 'bus' 'metro' 'metro_station' 'park' 'public_square'\n",
            " 'shopping_mall' 'street_pedestrian' 'street_traffic' 'tram']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "7\n",
            "street_pedestrian\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-9Xa4mHru9s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "freq = X[0].shape[0] #  no. of rows\n",
        "tm = X[0].shape[1] # no. of columns\n",
        "numClasses = len(labels)\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import LSTM, Dense, Flatten, Bidirectional, Permute, Reshape, Activation, add\n",
        "from keras.layers import Dropout, Conv2D, BatchNormalization, MaxPooling2D, TimeDistributed, AveragePooling2D, Input\n",
        "from keras.callbacks import *\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
        "\n",
        "#training parameters\n",
        "ep = 200  # number of epochs\n",
        "bs = 32  # batch size\n",
        "\n",
        "#callbacks settings\n",
        "def caller(name):\n",
        "  filepath= '/content/drive/My Drive/DCASE19 ASC/checkpoints/'+name+'.hdf5'\n",
        "  checkpoint = ModelCheckpoint(filepath, monitor='val_acc', mode='max',verbose=1, save_best_only=True)\n",
        "  es = EarlyStopping(monitor='val_acc',mode='max',patience=10,verbose=1,restore_best_weights=True)\n",
        "\n",
        "  callbacks_list = [es] #,checkpoint] \n",
        "  return callbacks_list\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-Y8qmPlmX7y",
        "colab_type": "code",
        "outputId": "a1eb422e-03b8-44f5-90fb-895c83219d73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9185, 117, 234, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zY0VWMoxgDaa",
        "colab_type": "text"
      },
      "source": [
        "# Model 1 - baseline\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YiYAAaiJhJIY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "acc=[]  #use to track results\n",
        "pre=[]\n",
        "rec=[]\n",
        "f=[]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upUT0WWqskY7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#X_ = np.transpose(X, (0,2,1,3))  #234 x 117\n",
        "#X_test_ = np.transpose(X_test, (0,2,1,3))\n",
        "\n",
        "drate = 0.5\n",
        "\n",
        "model = Sequential()  #initialize network \n",
        "\n",
        "model.add(Conv2D(filters=8, kernel_size=(3,3), activation='relu', input_shape=(freq, tm, 1)))  #original\n",
        "#model.add(Conv2D(filters=8, kernel_size=(3,3), activation='relu', input_shape=(X_.shape[1], X_.shape[2], X_.shape[3])))  #transposed input\n",
        "#model.add(Conv2D(filters=32, kernel_size=(7,7), activation='relu', input_shape=(freq,tm,1)))  #larger filter, kernel\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "#model.add(BatchNormalization())  \n",
        "\n",
        "model.add(Conv2D(filters=8, kernel_size=(3,3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "#model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv2D(filters=8, kernel_size=(3,3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "#model.add(BatchNormalization())\n",
        "\n",
        "\n",
        "model.add(TimeDistributed(Flatten()))  \n",
        "# lstm input needs to be 3D\n",
        "\n",
        "model.add(Dropout(drate))  \n",
        "\n",
        "model.add(Bidirectional(LSTM(40,return_sequences=True)))\n",
        "\n",
        "\n",
        "model.add(Dropout(drate))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(numClasses,activation='softmax'))\n",
        "\n",
        "callbacks_list = caller('model_')\n",
        "\n",
        "\n",
        "model.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['acc'])  #original default\n",
        "\n",
        "model.summary()\n",
        "\n",
        "\n",
        "\n",
        "history = model.fit(X, Y_cat, batch_size=32, epochs=100, validation_data=(X_test,Y_test_cat)) #, callbacks=callbacks_list)\n",
        "a,b,c,d = stats(model, None, X_test)\n",
        "acc.append(a)\n",
        "rec.append(b)\n",
        "pre.append(c)\n",
        "f.append(d)\n",
        "\n",
        "#history = model.fit(X_, Y_cat, batch_size=32, epochs=100, validation_data=(X_test_,Y_test_cat)) #, callbacks=callbacks_list)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h57nIyuVY8nq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(acc)\n",
        "print(rec)\n",
        "print(pre)\n",
        "print(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4X8QbeIFz1TN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ci(acc)\n",
        "ci(pre)\n",
        "ci(rec)\n",
        "ci(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lc1sKZ2KaZwL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_loss(history)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQbnlUMRo59u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# to get confusion matrix\n",
        "yhat_d = model.predict(X_test)\n",
        "pred = yhat_d.argmax(axis=1)\n",
        "cm_d = confusion_matrix(Y_test,pred)\n",
        "confmat(cm_d, labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_88WsIQgMuS",
        "colab_type": "text"
      },
      "source": [
        "# Model 1 with skip-connection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmYLRNoWFs9l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_this = Input(shape=(freq,tm,1))\n",
        "#input_this = Input(shape=(X_.shape[1],X_.shape[2],1))\n",
        "\n",
        "ins = Conv2D(filters=8, kernel_size=(3,3), activation='relu')(input_this)\n",
        "ins = MaxPooling2D(pool_size=(2,2))(ins)\n",
        "#ins = BatchNormalization()(ins)\n",
        "\n",
        "# 1 resiudal\n",
        "x = Conv2D(filters=8, kernel_size=(3,3), activation='relu', padding='same')(ins)  # res block, first conv.\n",
        "x = MaxPooling2D(pool_size=(2,2), strides=1, padding='same')(x)  #set strides=1 to retain output shape same as input shape\n",
        "#x = BatchNormalization()(x)\n",
        "\n",
        "x = Conv2D(filters=8, kernel_size=(3,3), activation='relu', padding='same')(x) #res block, 2nd conv\n",
        "x = MaxPooling2D(pool_size=(2,2), strides=1, padding='same')(x)\n",
        "#x = BatchNormalization()(x)\n",
        "\n",
        "#x = Conv2D(filters=8, kernel_size=(3,3), activation='relu', padding='same')(x) # block, 3rd conv\n",
        "#x = BatchNormalization()(x)\n",
        "\n",
        "x1 = add([ins,x])  # residual block output\n",
        "\n",
        "x1 = TimeDistributed(Flatten())(x1)\n",
        "\n",
        "x1 = Dropout(0.5)(x1)\n",
        "\n",
        "x1 = Bidirectional(LSTM(40, return_sequences=True))(x1)\n",
        "\n",
        "x1 = Dropout(0.5)(x1)\n",
        "\n",
        "y = Flatten()(x1)\n",
        "\n",
        "out = Dense(numClasses, activation='softmax')(y)\n",
        "\n",
        "\n",
        "model_minires = Model(inputs = input_this, outputs = out)\n",
        "\n",
        "model_minires.compile(optimizer='Adam', loss = 'categorical_crossentropy', metrics = ['acc'])\n",
        "model_minires.summary()\n",
        "callbacks_list = caller('model_minires')\n",
        "history_minires = model_minires.fit(X, Y_cat, batch_size=bs, epochs=100, validation_data=(X_test,Y_test_cat), callbacks=callbacks_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6FBQKDXPV_R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_loss(history_minires)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8v1vz1FPnDs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load a checkpoint model weights\n",
        "#model_minires.load_weights('/content/drive/My Drive/DCASE19 ASC/checkpoints/model_minires.hdf5')\n",
        "\n",
        "yhat_minires = model_minires.predict(X_test)\n",
        "pred_minires = yhat_minires.argmax(axis=1)\n",
        "cm_minires = confusion_matrix(Y_test,pred_minires)\n",
        "confmat(cm_minires, labels)\n",
        "print(accuracy_score(Y_test,pred_minires)*100)\n",
        "print(classification_report(Y_test,pred_minires,target_names=labels))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkwH7NrQ9mYI",
        "colab_type": "text"
      },
      "source": [
        "# Resnet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iByj-iARGCQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# resnet like model. keras functional\n",
        "\n",
        "input_this = Input(shape=(freq,tm,1))\n",
        "\n",
        "ins = Conv2D(filters=8, kernel_size=(3,3), activation='relu')(input_this)\n",
        "#ins = MaxPooling2D(pool_size=(2,2))(ins)\n",
        "ins = BatchNormalization()(ins)\n",
        "\n",
        "# Residual block 1\n",
        "x = Conv2D(filters=8, kernel_size=(3,3), padding='same', activation='relu')(ins)  #first res block, first conv\n",
        "#x = MaxPooling2D(pool_size=(2,2),strides=1, padding='same')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv2D(filters=8, kernel_size=(3,3), padding='same', activation='relu')(x) #first block, 2nd conv\n",
        "#x = MaxPooling2D(pool_size=(2,2),strides=1, padding='same')(x)\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "\n",
        "x1 = add([ins,x])  #add two outputs/inputs for residual. First residual block output\n",
        "\n",
        "x1 = Dropout(0.5)(x1)\n",
        "\n",
        "y = Flatten()(x1)\n",
        "\n",
        "out = Dense(numClasses, activation='softmax')(y)\n",
        "\n",
        "model_res = Model(inputs = input_this, outputs = out)\n",
        "\n",
        "model_res.compile(optimizer='Adam', loss = 'categorical_crossentropy', metrics = ['acc'])\n",
        "model_res.summary()\n",
        "callbacks_list = caller('model_res')\n",
        "history_res = model_res.fit(X, Y_cat, batch_size=bs, epochs=100, validation_data=(X_test,Y_test_cat), callbacks=callbacks_list)\n",
        "a,b,c,d = stats(model_res, None, X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5okyDWphEr03",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_loss(history_res)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EE0clTZExQ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model_res.load_weights('/content/drive/My Drive/DCASE19 ASC/checkpoints/model_res.hdf5')\n",
        "\n",
        "yhat_res = model_res.predict(X_test)\n",
        "pred_res = yhat_res.argmax(axis=1)\n",
        "cm_res = confusion_matrix(Y_test,pred_res)\n",
        "confmat(cm_res, labels)\n",
        "print(accuracy_score(Y_test,pred_res)*100)\n",
        "print(classification_report(Y_test,pred_res,target_names=labels))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wL8hBHsPjRjk",
        "colab_type": "text"
      },
      "source": [
        "# CNN\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vh5CTOTNjX31",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "modeld = Sequential()\n",
        "\n",
        "modeld.add(Conv2D(filters=8, kernel_size=(3,3), activation='relu', input_shape=(freq,tm,1)))\n",
        "modeld.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "modeld.add(Conv2D(filters=8, kernel_size=(3,3), activation='relu'))\n",
        "modeld.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "modeld.add(Conv2D(filters=8, kernel_size=(3,3), activation='relu'))\n",
        "modeld.add(MaxPooling2D(pool_size=(2,2)))  #CHANGED: was (4,100) (too big) just scaled down to (1,25)\n",
        "modeld.add(Dropout(0.5))\n",
        "\n",
        "modeld.add(Flatten())\n",
        "\n",
        "modeld.add(Dense(numClasses, activation='softmax'))\n",
        "\n",
        "modeld.compile(optimizer='Adam', loss = 'categorical_crossentropy', metrics = ['acc'])\n",
        "modeld.summary()\n",
        "callbacks_list = caller('modelcnn')\n",
        "history_modeld = modeld.fit(X, Y_cat, batch_size=bs, epochs=100, validation_data=(X_test,Y_test_cat),callbacks=callbacks_list)\n",
        "ac,bc,cc,dc = stats(modeld, None, X_test)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rNBKQ5WE05V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_loss(history_modeld)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hh1o0t2HFECP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#modeld.load_weights('/content/drive/My Drive/DCASE19 ASC/checkpoints/modeld.hdf5')\n",
        "\n",
        "yhat_d = modeld.predict(X_test)\n",
        "pred_d = yhat_d.argmax(axis=1)\n",
        "cm_d = confusion_matrix(Y_test,pred_d)\n",
        "confmat(cm_d, labels)\n",
        "print(accuracy_score(Y_test,pred_d)*100)\n",
        "print(classification_report(Y_test,pred_d,target_names=labels))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NO9dT2c5nUcc",
        "colab_type": "text"
      },
      "source": [
        "# RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIcGjO2PDPls",
        "colab_type": "code",
        "outputId": "c3ae09be-fa67-42b1-848b-332fa3af5d3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# RESHAPE THE INPUT DIMENSIONS FOR LSTM USE (REMOVE THE CHANNEL DIMENSION)\n",
        "Xr = np.reshape(X, (len(X), freq,tm) )\n",
        "Xr = np.transpose(Xr, (0,2,1))\n",
        "X_testr = np.reshape(X_test, (len(X_test),freq,tm))\n",
        "X_testr = np.transpose(X_testr, (0,2,1))\n",
        "# NOTE: TRY TRANSPOSING SO TIME DIMENSION IS FIRST SO INPUT SHAPE IS (BATCH, TIMESTEPS, INPUT DIME)\n",
        "print(Xr.shape[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "234\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkGa461OnciJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_rnn = Sequential()\n",
        "\n",
        "model_rnn.add(Bidirectional(LSTM(40, return_sequences=True), input_shape=(Xr.shape[1],Xr.shape[2])))  #lstm 1\n",
        "\n",
        "\n",
        "model_rnn.add(Flatten())\n",
        "model_rnn.add(Dense(numClasses, activation='softmax'))\n",
        "\n",
        "model_rnn.compile(optimizer='Adam', loss = 'categorical_crossentropy', metrics = ['acc'])\n",
        "model_rnn.summary()\n",
        "callbacks_list = caller('model_rnn')\n",
        "\n",
        "\n",
        "history_rnn = model_rnn.fit(Xr, Y_cat, batch_size=bs, epochs=25, validation_data=(X_testr,Y_test_cat),callbacks=callbacks_list)\n",
        "ac,bc,cc,dc = stats(model_rnn, None, X_testr)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwJeAOPqnXdM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_loss(history_rnn)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}